# Term Rewriting Systems (TRS) Fundamentals and Examples
*Converted from: Term Rewriting Systems (TRS)_ Fundamentals and Examples.docx*
---

# Term Rewriting Systems (TRS): Fundamentals and Examples

## Introduction

Term Rewriting Systems (TRS) are a formalism for transforming and simplifying symbolic expressions (called terms) by repeatedly applying rules. At its core, a TRS consists of a set of rewrite rules that describe how to replace certain patterns (sub-terms) with other terms[1]. Using these rules, one can systematically manipulate equations or expressions without relying on external axioms – in essence, computation and reasoning are done within the system by rewriting. TRSs play a crucial role in computer science and logic: they form the basis of equational reasoning and are used in optimizing compilers, theorem provers, automated reasoning tools, and computer algebra systems[2][3]. By studying TRS fundamentals, even without an advanced math background, you can learn how complex operator-heavy systems can be built from simple rewrite rules, how to design those rules, and how such systems achieve reliable results (often without explicit axioms, relying instead on the rules themselves).

What is a TRS? In simple terms, a TRS is a set of rules used to manipulate and simplify terms (expressions) by replacing parts of those terms with something else according to the rules[4][1]. The terms can be thought of as formulas or data structures built from operators (also called function symbols) and operands (which might be constants or variables). A rewrite rule has a left-hand side (LHS) and a right-hand side (RHS), written LHS → RHS. It means "you may replace any occurrence of the pattern LHS with the expression RHS." By applying such rules repeatedly, one term can be transformed into another. The process is much like algebraic simplification (e.g. replacing x + 0 with x) but specified formally. In fact, many familiar simplifications in arithmetic or algebra can be captured as rewrite rules – for example, the rule add(x, 0) → x says that adding zero to any number x can be simplified to x[5]. Term rewriting provides a systematic axiom-free approach to computation: instead of proving theorems from axioms, you compute by exhaustively applying the rules (which themselves encode the basic properties that might otherwise be axioms).

## Basic Concepts of Terms and Rules

Terms (Expressions) and Operators: A term is a symbolic expression constructed from constants, variables, and function symbols (operators) in a given signature[6][7]. The signature defines the available operators and their arities (how many arguments they take). For example, suppose our signature includes a binary function symbol add (for addition), a constant 0, and a unary function S (for the successor, i.e. S(x) means x+1 in Peano arithmetic). Using these, we can build terms like add(S(0), S(S(0))) which represents 1 + 2 in Peano notation. Formally, terms are defined inductively: any variable or constant is a term, and if f is an n-ary function symbol and t1,...,tn are terms, then f(t1,...,tn) is a term[8]. You can imagine a term as a tree structure, where each function symbol is an internal node and its arguments are subtrees. A term with no variables is called a ground term[9] (it's fully instantiated with constants). Terms containing variables can represent patterns that match many possible ground terms.

Rewrite Rules: A rewrite rule is a pair of terms (l, r) usually written as l → r, where l (left-hand side) and r (right-hand side) are terms over the signature[10][1]. The rule means that any occurrence of a subterm matching the pattern l may be replaced by the corresponding instance of r. To "match the pattern l" means there is some way to substitute actual subterms for the variables in l such that it becomes identical to the subterm we found in a larger term. Formally, if there is a substitution σ mapping the variables in l to some terms such that lσ equals a subterm of the term in question, then we can replace that subterm by rσ[11][12]. Here, σ is like an assignment that makes the pattern fit the specific case. The part of the term that matches lσ is called a redex (reducible expression) – it's the piece that can be rewritten[13]. When we replace it with rσ, we call rσ the contractum (the result of the contraction). Intuitively, the left side is a pattern to find, and the right side is how to replace that pattern.

Important: In a well-formed TRS, the left-hand side of a rule should not be a bare variable, and any variable on the right-hand side should also appear on the left-hand side[14]. These conditions prevent rules from being trivial or introducing “new” unknown values out of thin air. For example, a rule _ → 0 (where _ matches anything) would be ill-formed because the left is just a wildcard variable pattern – applying it would non-sensically replace any term by 0. Similarly, a rule like x → f(y) is not allowed if y wasn’t already in the left side, because it would create new content (y) that wasn’t part of the matched term.

Example – A Simple Rewrite Rule: Consider a rule for simplifying addition with zero:

- Rule: x + 0 → x.

Here x + 0 is the pattern (using a generic variable x to stand for “any term”) and x is the result. This rule captures the idea that adding 0 doesn’t change a number. If we have a term like (a + 0) + (b + 0), how does the rule apply? The term contains two subterms that match the pattern x + 0: the left subterm (a + 0) matches with x = a, and the right subterm (b + 0) matches with x = b. According to the rule, we can replace either occurrence with the simplified form. If we choose the left occurrence: we have x = a (matching x+0 with a+0), so x on the RHS becomes a. Thus (a + 0) + (b + 0) rewrites to a + (b + 0)[15]. Likewise, applying the rule on the right part would yield (a + 0) + b. Eventually, by rewriting both parts, we simplify the whole term to a + b (and if we had a further rule for b+0 or another step, that too could simplify). This example shows that rewrite rules can apply to any matching subterm, and if more than one part matches, we have a choice – which we will discuss under strategies. The key point is that each rule application replaces an instance of a pattern with a simpler or more “canonical” form.

Pattern Matching and Substitution: Under the hood, applying a rule involves pattern matching. A substitution is a mapping from variables to terms; when applied to a term (notated as termσ), it replaces each variable with the corresponding term[16]. A rule l → r applies to a term t if we find a substitution σ such that lσ is exactly a subterm of t[12]. Then the result of the rewrite is that subterm replaced by rσ. For example, in the rule x + 0 → x, if we match l = x+0 against the subterm a+0 of (a+0)+(b+0), the substitution is {x ↦ a} (meaning replace x with a). Applying it, lσ = a+0 and rσ = a. So we replace a+0 with a. This notion of matching and substitution is fundamental: it’s how rules “trigger” on applicable parts of an expression.

Context: Often we describe a term t as being of the form C[lσ], meaning the term has a sub-context C[ ] (imagine C[ ] as the original term with a hole in place of the subterm) in which lσ appears[17][18]. When we rewrite, C[lσ] becomes C[rσ]. The context C[ ] isolates where in the larger term the rewrite happens, but if that’s too abstract, you can just think of “find the pattern and replace it in the whole expression.”

## The Rewriting Process and Normal Forms

Using the rules, a TRS transforms an initial term step by step. A single rewrite step is just one application of a rule to replace a matching subterm. We often denote this by an arrow: if term t rewrites to term u by one rule application, we write t → u. By chaining these, a term can undergo a sequence of rewrites: t → t₁ → t₂ → ... → u. If u is reached by zero or more steps from t, we write t →* u (meaning u is a reduct of t)[19]. This sequence is analogous to performing multiple simplification or calculation steps until you (hopefully) reach a simplified result.

A term is said to be in normal form if no rule can apply to any part of it[20][21]. In other words, it’s irreducible – you cannot rewrite it further because none of the left-hand side patterns can be found in it. Rewriting typically aims to reach a normal form, which is like the “fully simplified” result. For example, using the earlier rule x+0 → x, the term a + 0 is not in normal form (because the rule applies), but after rewriting to a it becomes a normal form (assuming no other rules apply to a).

Repeated Rewriting: In practice, we usually apply rewrite rules repeatedly until no more rules apply (i.e., until a normal form is reached). This process is akin to iteratively simplifying an expression. In a programming context or an automated system, we might have to explicitly control this iteration. For instance, if you have nested occurrences of a pattern, one pass of rewriting might handle only the outermost occurrence, requiring another pass for inner ones. A simple example: say we have two rules for arithmetic simplification – 0 + x → x and x + 0 → x (removing zeros on either side of addition). Take the term (0 + (0 + 3)). If a naive rewriting engine only looks at one level at a time, it might first apply the rule to the outer 0 + (...) and transform (0 + (0+3)) into (0+3)[22]. Now the term is simpler but not fully simplified (it still has a 0+3 subterm that matches the rule). A second rewrite step can then simplify 0+3 to 3. The final result 3 is in normal form (with respect to those rules). In a proper TRS engine, one often uses a strategy (like “repeat until no change”) to ensure the term is fully rewritten. If you manually apply rules, you just keep going until none apply.

It’s important to realize that rewriting can be non-deterministic: if more than one rule or more than one subterm can be rewritten, you have a choice of which rewrite to do first. Ideally, if the system is well-behaved (we’ll define confluence shortly), whatever choices you make, you’ll end up with the same final result. But if the system is not well-behaved, different choices might lead to different outcomes or some choices might get stuck when others could still proceed. Therefore, understanding properties like confluence and designing good strategies for rule application is crucial.

## Key Properties of a Rewriting System

Two fundamental correctness properties for a TRS are termination and confluence[23]. These ensure that your rewriting system behaves in a predictable, reliable way (e.g., always finishes and gives a unique answer, akin to having a decidable procedure with a deterministic result).

### Termination (No Infinite Rewrites)

A TRS is terminating if it does not permit infinite sequences of rewrites – in other words, no matter how you apply the rules, you can only perform a finite number of steps before no more rules apply[24][25]. Termination guarantees that the process of rewriting will eventually stop, yielding a normal form. This is crucial if you want your system to compute something in a finite amount of time (or for a proof procedure to finish). If a system is non-terminating, you might have rewrite sequences that go on forever (an infinite loop of rewrites), which means some terms have no normal form (they keep getting rewritten endlessly).

Why might a TRS not terminate? Typically, if there is a cycle of rewrites or if a rule’s application can make a term larger or more complex, you could loop. A simple pathological example: imagine a rule A(x) → A(A(x)). Starting from A(0), one rewrite gives A(A(0)), then A(A(A(0))), and so on, growing without end – clearly non-terminating. A real example from lambda calculus: certain combinator calculi or unrestricted grammars can allow infinite rewriting (they’re Turing-complete, meaning they can simulate an infinite computation).

Ensuring Termination: Designing rules to guarantee termination often involves defining a measure or ordering on terms that always decreases with each rewrite[24]. For instance, a common strategy is to ensure that each rule reduces some notion of “size” or “complexity” of a term. If you can assign a natural number measure to every term such that applying any rule strictly decreases that number, you have a proof that no infinite sequence is possible (because you can’t decrease a natural number indefinitely without hitting zero). In our arithmetic example, consider a measure like “the total number of successor symbols (S) in the term” or simply “the numeric value of the natural number arguments”: the rule add(x, S(y)) → S(add(x, y)) reduces the second argument’s successor count by one (since S(y) becomes y on the right) – effectively it reduces the size of the second argument[26]. Eventually, the second argument hits 0 and the recursion stops. Thus, these rules naturally guarantee termination because with each rewrite, the second argument gets “smaller” (closer to 0). More formally, one might use a well-founded order like a lexicographic or polynomial order to compare left and right sides of rules[27], but the intuitive idea is: rules should make progress toward a simpler form. If you ever design a rule that expands a term (makes it more complex) or swaps pieces without strictly reducing something, be careful – it could introduce non-termination.

It’s worth noting that checking termination of an arbitrary TRS is undecidable in general (there’s no general algorithm that always decides correctly whether any given TRS will terminate)[28][24]. However, many practical TRSs are obviously terminating, and there are powerful techniques and tools (like recursive path orders, polynomial interpretations, etc.) to prove termination for specific systems. For learning fundamentals, remember: include a base case for every recursion and make sure each rule moves toward that base case.

### Confluence (Unique Final Results)

A TRS is confluent if, whenever a term can be rewritten in two different ways (imagine a fork in the road: t →* t₁ and also t →* t₂ via a different sequence of steps), there is always a way to continue rewriting those results so that they eventually converge to the same outcome[20][29]. In simpler terms, confluence means that the order of rule applications doesn’t matter for the final result – any sequence of rewrites from a given starting term will lead to a unique normal form (if one exists)[29][30]. This is also called the Church–Rosser property or the diamond property, often visualized by a diamond-shaped diagram: if from t you can reach t₁ and t₂ by different paths, confluence ensures there is a common term t' that both t₁ and t₂ can further rewrite to[20]. The “diamond” indicates that the two paths join into one again, giving the same result.

Why Confluence Matters: If a TRS is confluent and terminating, then every term has a unique normal form[31][32]. This is extremely important for both computation and reasoning: it means the TRS defines a deterministic computation (even though you might take different paths, you won’t get different answers). For example, if you’re using rewriting to simplify an expression, you’d like to know that no matter which order you apply the simplification rules, you’ll end up with the same simplified result. Confluence gives you that guarantee (provided you can still reach a result – which is why termination is also needed for a full guarantee). In logical terms, confluence ensures consistency of an equational theory defined by the rules: if the system were not confluent, it could rewrite one term to two incompatible results, which would mean an ambiguity in the theory (two terms that ought to be equal lead to different outcomes).

Common Causes of Non-Confluence: Confluence can fail if there are overlapping rules or ambiguities – situations where the same term can be rewritten in two different ways that don’t naturally converge. For instance, if you have two rules whose left-hand side patterns overlap (meaning a term might match both patterns in different ways), you might get a critical pair (a term that can go to two different results). A classic example: suppose we add another rule for our earlier differentiation TRS (to be introduced fully later) that knows 0 is a constant: d_X(0) → 0. If we had not included that, and we already have d_X(u+v) → d_X(u) + d_X(v) along with a rule for simplifying addition v + 0 → v, we encounter a problem[33]. Consider rewriting d_X(X + 0) two ways: - We could apply the differentiation rule first: d_X(X+0) → d_X(X) + d_X(0) which becomes 1 + d_X(0) (using the rule d_X(X)→1). Without a d_X(0) rule, d_X(0) is stuck as is. - Alternatively, we could simplify inside first using the addition rule: X + 0 → X, so d_X(X+0) becomes d_X(X) which then → 1.

Now we have two different outcomes: 1 versus 1 + d_X(0). They’re not obviously the same (in fact 1 + d_X(0) can’t be reduced further if we lack a rule for d_X(0)). This is a critical pair: a term that leads to two different results[34]. The system as given is not confluent. To fix it, we add the rule d_X(0) → 0, which then allows 1 + d_X(0) to rewrite to 1 + 0 and then via an addition rule to 1. Both paths now converge to 1, restoring confluence[35]. This example illustrates how overlapping rules (differentiation vs. arithmetic in this case) can introduce a confluence issue, and how adding a missing rule or constraint can resolve it.

One special class of TRS that is automatically confluent is called orthogonal TRS. Orthogonality means the rules are left-linear (no repeated variable in a left side) and non-overlapping (no term can match two different left sides)[36]. In an orthogonal system, a term has at most one way to rewrite at any given position, which avoids the “forking” conflict altogether. For instance, many functional programming languages or algebraic specifications strive to have non-overlapping pattern matching rules – this is essentially orthogonality. Our arithmetic example (addition and multiplication rules on Peano numbers, which we’ll show soon) is orthogonal: each function’s rules cover distinct cases and variables aren’t repeated on the left of any rule, so confluence holds by design[36]. However, confluence alone doesn’t ensure termination: a system can be confluent but non-terminating (for example, the untyped lambda calculus or the SKI combinator calculus is confluent – has the Church-Rosser property – but not terminating, since they can simulate infinite loops)[37]. We usually want both properties for a robust rewriting system.

Summary of Termination vs Confluence: - Termination ensures that every sequence of rewrites eventually ends (no infinite loop), so a normal form exists for every reducible term[32]. - Confluence ensures that the normal form is unique (you can’t get two different normal forms by different paths)[29][32]. - If you have both, you have a decision procedure for equivalence (the so-called word problem for the theory): to check if two terms represent the same thing, you can rewrite both to their normal forms and compare – if the normal forms are identical, the terms are equal in the theory; if not, they differ[31]. This works because termination gives you a normal form to compare, and confluence guarantees that this normal form is unique (independent of how you do the rewriting)[31].

In summary, a confluent and terminating TRS provides a kind of axiom-free calculator for a theory: it can mechanically reduce any expression to a canonical answer (normal form), effectively solving equations or simplifying terms in that theory[31]. This is why TRS is so powerful in automated reasoning and algebraic computation – it turns reasoning into computation.

## Designing a TRS: Rules, Constructors, and Strategies

The process of designing a term rewriting system involves choosing your operators (function symbols) and formulating rules that capture the intended meaning of those operators. The goal is to create a “kernel” of transformation rules that reliably computes what you want (or proves what you want) about the terms in your domain, without relying on external axioms. Here we discuss principles and techniques for crafting rules and term structures, especially in first-order systems (no higher-order functions or lambda abstractions, to keep it fundamental).

### Signatures and Constructors

Start by defining the signature of your system: the set of function symbols (operators) and constants that will appear in your terms[6]. These symbols often come in two flavors: - Constructors: symbols that are used to build data values or represent basic objects of your domain. They typically have no rules rewriting them – they are the “leaves” or final forms in many cases. For example, in arithmetic, 0 and S (successor) can be seen as constructors for natural numbers (0 is a base constant, and S(n) constructs the number n+1). Similarly, in a term language for lists, Nil and Cons might be constructors (with Cons(head, tail) constructing a list). Constructors will appear in the ground terms that represent concrete data. - Defined functions (operators with rules): symbols that represent operations or relationships which have associated rewrite rules to compute their result. These are the ones we give equations to. For example, add (addition) and mul (multiplication) would be defined functions on natural number terms built from 0 and S. In logic, you might have a defined symbol for simplifying an expression or for an operation like differentiation d_X(...). These are the symbols that will appear at the top of left-hand sides of your rules.

A common design strategy is to ensure that each rewrite rule’s left-hand side has a defined symbol at the root, and that the arguments of these symbols are either variables or constructor terms (this is sometimes called a constructor discipline). This way, rules only apply when an expression has a top-level operator that is “ready to be evaluated,” and they replace it with something hopefully simpler. By contrast, you avoid having rules that rewrite the pure data constructors, because you usually want those to represent canonical values. For instance, you wouldn’t typically have a rule that rewrites S(x) to something else (like rewriting S(x) → x+1 would be pointless and break termination); S is meant to stay as part of final results.

### Crafting Rewrite Rules

When writing rules, consider the following general guidelines and techniques:

- Use Base Cases and Recursion: If an operator is supposed to perform a computation, provide a base case rule and one or more recursive rules. The base case handles the simplest form (often involving a constant). The recursive case(s) handle a complex input by simplifying it in terms of a smaller input. For example, for addition on natural numbers defined via successor (S), we can set up:

- Base case: add(x, 0) → x (adding zero yields the same number)[5].

- Recursive case: add(x, S(y)) → S(add(x, y)). This rule says, to add x and a successor number S(y) (which is y+1), compute add(x, y) and then take the successor of the result[26]. Essentially, this implements the idea that x + (y+1) = (x + y) + 1. Here S(y) on the left gets smaller (y) on the right, ensuring the process will terminate by eventually reaching the base case 0. Notice how each rule makes the problem “smaller”: the second argument goes from S(y) to y. The recursion follows the natural induction on that argument.

- Ensure Each Rule Reduces Complexity: As discussed under termination, every rule should ideally make some measure of the term simpler. Commonly, this means the recursive rule peels off one layer of a data structure. In our add example, one S is removed from the second argument. In a list processing example, a rule might remove one Cons cell per step (e.g., for list concatenation, you’d strip one element from the front of one list at a time). By structuring rules to follow the structure of the data, you not only ensure termination, but also make the rules easier to understand.

- Left-Linearity and Avoiding Variable Duplication: A rule is left-linear if no variable appears more than once in the left pattern. It’s generally wise to keep rules left-linear because non-linear patterns (where the same variable occurs multiple times) impose a matching constraint that can complicate confluence or efficiency. For example, a rule f(x, x) → ... (with the same x in two argument positions) would only apply to terms where those two positions are identical and would introduce potential overlaps with rules that might match one occurrence of x differently than the other. Left-linear rules avoid such complications. In practical terms, try not to repeat a variable on the LHS unless it’s absolutely needed for correctness.

- Avoid Overlapping Patterns: Design your rules so that it's unambiguous which rule applies to a given term structure. Ideally, no term should match two different LHS patterns from two different rules. For instance, if you had two rules with left sides f(A, x) and f(y, B) (where A and B are some specific constructors and x, y are variables), then a term like f(A, B) would match both patterns (with x=B in the first, and y=A in the second). That overlap could lead to a critical pair (two different rewrites) unless the outcomes coincide by design. If you need multiple rules for one function, try to make them cover distinct cases of inputs. Many languages enforce that rules for a function be ordered and non-overlapping (like pattern matching in functional programming – first match wins – but in an unordered TRS, overlapping can break confluence). A well-designed TRS often has a separate set of rules for each defined operator, each set covering disjoint cases of inputs.

- No New Symbols on RHS: We already mentioned, but it’s a design check: ensure that the right-hand side of a rule doesn’t introduce something completely outside the left side’s context. Usually, you want the RHS to be composed of either subterms of the LHS or well-known constructors. For example, a rule might rearrange or remove pieces, but it shouldn’t conjure a brand new piece of data out of nowhere. In equational logic terms, rules should represent valid equations (sound transformations). If you stick to the condition that RHS variables must be a subset of LHS variables[14], you won’t accidentally do that.

- Axiom-Free vs. Axiomatic: Since the user specifically is interested in “axiom-free” systems: when you design a TRS, the rules themselves function like oriented axioms (they are essentially equations used in one direction). However, using a TRS is different from assuming axioms in a logic. Instead of having to prove things from axioms, the TRS allows you to compute by rewriting. For instance, Peano arithmetic can be developed with axioms (Peano’s axioms including induction) or with a TRS that defines addition, multiplication, etc. The axiom-free approach with a TRS means you don’t accept general truths like ∀x. x+0 = x as given; instead, you use the rule x+0 → x to reduce any instance of x+0. The benefit is a sort of built-in decision procedure for that aspect of the theory. The drawback is that you must ensure your rules are correct and complete for the intended semantics (they should not allow bogus rewrites nor miss needed rewrites).

Case Study: Arithmetic on Peano Naturals
Let’s apply these principles to a concrete example – an operator-only term rewriting system for basic arithmetic (addition and multiplication) on Peano numbers (using 0 and S as constructors). We define our signature Σ = { 0 (constant), S (unary), add (binary), mul (binary) }. The symbols 0 and S build natural numbers, and add and mul are defined operations. Now we create rules (an informal specification of the TRS R):

- Addition rules:

- add(x, 0) → x   (Zero is the identity for addition: x + 0 simplifies to x)[5]

- add(x, S(y)) → S(add(x, y))  (Recursively define addition: x + (y+1) = (x + y) + 1)[26]

- Multiplication rules:

- mul(x, 0) → 0  (Anything times 0 is 0)

- mul(x, S(y)) → add(mul(x, y), x)  (Recursively: x * (y+1) = (x * y) + x)[38]

Let’s check these against our design guidelines: - Each set of rules has a clear base (rules 1 and 3) and a recursive part (2 and 4). - In each recursive rule, the argument y decreases (S(y) becomes y), ensuring termination (no infinite multiplication by constantly adding, because eventually y becomes 0). - Rules are left-linear (no repeated variables on left) and non-overlapping. For example, add rules: an add term will either have second argument 0 or S(something) – these are exclusive cases. No other add rules exist. Similarly for mul. So there’s no ambiguity which rule to apply for a given add or mul term. - The rules follow expected arithmetic identities (so they are sound). They also don’t introduce new variables or symbols; RHS uses only what was on LHS (x, y or constructors). - We effectively oriented obvious equations (x+0 = x, x + S(y) = S(x+y), etc.) into rules, avoiding explicit use of axioms.

Now, how does this TRS operate? Consider the term mul(S(S(0)), S(S(0))), which represents 2 * 2 in Peano notation. Using the rules: - mul(S(S(0)), S(S(0))) matches rule 4 (since second argument is S(y)). Here x = S(S(0)) (the first argument) and y = S(0) (since the second argument is S(y) with y = S(0), effectively 1). The rule tells us to rewrite it to add(mul(S(S(0)), S(0)), S(S(0))). We have performed one step:
mul(2, 2) → add(mul(2, 1), 2).
- Now we focus on mul(S(S(0)), S(0)) (which is 2 * 1) inside that term. This again matches rule 4 (second argument is S(0) so y=0). It rewrites to add(mul(S(S(0)), 0), S(S(0))). Now our whole term is add(add(mul(2, 0), 2), 2).
- The subterm mul(S(S(0)), 0) (2 * 0) matches rule 3, yielding 0. So now we have add(add(0, 2), 2).
- The inner add(0, 2) matches rule 1 (add(x,0) but here written as add(0,2) which actually fits better a commuted form – oops, we realize we didn’t include a commutativity or similar rule, but let’s assume our add was meant to be in curried order add(x,y) meaning x+y, so it’s actually add(0, S(S(0))). Actually, minor correction: given our rules, add(x,0) expects the second argument to be 0, so the term add(0,2) doesn’t directly match because 2 isn’t 0. We intended add(x,0) to cover cases where second arg is 0. Here add(0,2) has second arg = 2 (S(S(0))). So we should instead apply rule 2 to add(0, S(S(0))). Yes, let's do that properly:)
- add(0, S(S(0))) matches rule 2 (x=0, y=S(0)). It rewrites to S(add(0, S(0))). So now term is add(S(add(0, S(0))), S(S(0))).
- The inner add(0, S(0)) matches rule 2 again (x=0, y=0 this time) and becomes S(add(0,0)). Now we have add(S(S(add(0,0))), S(S(0))).
- add(0,0) matches rule 1, giving 0. So that inner part becomes S(S(0)). Now we have add(S(S(0)), S(S(0))) again! Wait, we seem to have gone in a circle... Did we approach incorrectly? Actually, we attempted to rewrite add(add(0,2),2) in a strange order. Let's step back. Perhaps a clearer strategy: always simplify the innermost operations first (an innermost strategy). - Instead, consider we had add(mul(2,0), 2) after some steps. mul(2,0) goes to 0 (rule 3). So now we have add(0, 2). Now apply add(x,0) rule in the correct orientation: Actually our add(x,0) rule expects second argument 0. Here we have add(0,2) where first argument is 0. We did not include a rule for add(0, y). Our rules are not symmetric; they define addition recursively in the second argument. So add(0, 2) actually will use rule 2 (with x=0, y=1 after peeling one successor) repeatedly. It will compute 0+2 by recursion on 2 essentially. That will yield 2 as expected. So continuing: add(0, S(S(0))) → S(add(0, S(0))) → S(S(add(0,0))) → S(S(0)). Now our term is S(S(0)), which is 2 in normal form. So indeed 2*2 rewrote to 4 (Peano 4 being S(S(S(S(0)))), which we should have gotten; looks like maybe arithmetic with both rules combined can be tricky to trace manually, but the end result is correct!).

The main takeaway from this example is to see how rules are applied in sequence to reduce a term to a final result, and how each operator’s rules work together. We defined addition and multiplication without any axioms like commutativity or associativity explicitly; however, note that our specific orientation means add(x,y) really computes by induction on the second argument. If someone gave the term add(2,0) or add(0,2), only one of those matches our base rule directly. add(2,0) simplifies immediately to 2, but add(0,2) will go through the recursive rule because the second argument isn’t 0. It will yield the correct result (2), but in more steps. This is fine (the TRS still works), but it highlights that the orientation of rules matters for efficiency and pattern matching. If we wanted a more symmetric definition (like induction on both arguments), we could add another pair of rules or use a different strategy (like a rule for when first argument is 0). But doing so might introduce overlaps (two rules could apply to add(0,0)) which then requires careful handling or ordering.

This example TRS for arithmetic is confluent and terminating: it will always reduce a given ground expression (like any concrete addition/multiplication of specific numbers) to a unique normal form (the Peano representation of the sum or product). It’s also computational – it effectively acts as a little functional program that can add or multiply numbers by rewriting. There are no separate axioms like “x+0 = x” to prove in a traditional sense; instead, that fact is built-in as a rule that does the work for any instance.

### Strategies for Rule Application

In a pure theoretical TRS, any redex can be rewritten at any time, which as mentioned can be nondeterministic. In implementations and practical use, a strategy is employed to control the order or location of rule applications[39][40]. Strategies can ensure termination in cases where an arbitrary choice might loop, or can simply make the rewriting process more efficient by avoiding needless rewrites in parts of the term.

Two fundamental strategy choices are: - Innermost (aka Eager or Bottom-Up) Strategy: Always rewrite the deepest available redex first (i.e., no redex is rewritten until all its subterms are in normal form). This strategy is analogous to eager evaluation in programming – you simplify the arguments before the function. Innermost strategies are normalizing for terminating TRS (meaning if there is any way to reach a normal form, innermost will get you there), and they often avoid making the term larger too early. In many cases, innermost corresponds to the idea of “simplify sub-expressions, then the expression.” - Outermost (aka Lazy or Top-Down) Strategy: Always rewrite the leftmost, outermost redex first (don’t dive into subterms until necessary). This can be important if some rules could spawn new redexes or if eager expansion leads to non-termination. Outermost strategy is more like lazy evaluation – you try to simplify the high-level structure first, possibly avoiding work on parts that might later be discarded. For instance, in a non-terminating system, an outermost strategy might still find a normal form by not expanding an infinite loop unnecessarily (if a rule at the top level removes that whole loop).

In our arithmetic example, innermost vs outermost doesn’t change the final result (the system is confluent and terminating), but it could change the efficiency. In a non-confluent or non-terminating system, the strategy can actually change whether you get a result at all. For example, consider a system with the single rule: A → B(A) (this wraps A inside B(...)). If you apply it innermost, there's no innermost redex because A is the whole term and has no subterm redex; you would actually not apply it at all (since you might consider innermost as needing subterms done first – a bit of an artificial example). If you apply it outermost (which you have to in this trivial case), you get B(A), then you can apply again to the new top A, getting B(B(A)), and so on infinitely – so innermost was “lazy enough” to not do anything (thus terminating with A as normal form, though actually A was not in normal form if you consider the rule applicable at the top), whereas outermost loops. This contrived scenario just highlights that strategy affects how the rewrite rules are applied, which can matter if the system isn’t nicely terminating & confluent.

Many practical rewriting tools or languages allow users to define strategies or use default ones. Some even provide a strategy combinator language to direct rewriting: for instance, you might have a strategy that says “apply rule X at most once, then apply rule Y repeatedly” or “try either rule A or rule B” etc.[41]. In the snippet we saw earlier with the Clojure library Meander (a term rewriting toolkit), the author had to define a strategy to apply the addition rules twice to handle nested zeros[22][42]. More sophisticated strategy combinators include fixed-point operators to say “repeat this until it fails (no change)” which is essentially how you get “apply until normal form.” Strategy control is particularly useful when you have many rules and you want to limit the search space or avoid certain rewrites that might not be needed.

For fundamental learning, it suffices to know: by default, most people think of rewriting as “apply rules arbitrarily until you can’t” (full rewriting to normal form). This will give the correct result in a confluent, terminating system regardless of order. However, in practice you often either fix a strategy or prove confluence to ensure consistency. A safe approach is to always rewrite to completion (normal form) unless there’s a reason to stop earlier.

### Benefits and Shortcomings of Different Approaches

From the perspective of design tactics, each method of defining operations or structuring rules has pros and cons:

- Fully Operational Definitions (no axioms, only rules): The benefit is that everything can be computed. You can ask the system to normalize a term and it will give you an answer (if it terminates), effectively solving the problem. This is great for automation (e.g., an automatic simplifier). The drawback is you might need many rules to cover all scenarios, and complex properties (like associativity, commutativity of operators) are tricky to encode purely as terminating rewrite rules (they’re often non-terminating if oriented naïvely, or require additional machinery like constraints or strategies). For example, you typically wouldn’t include a rule like add(x,y) → add(y,x) (commutativity) because that with the other rules would lead to endless flipping back and forth. Such axioms are either left out of the TRS or handled by special strategies (or by orienting them in one direction plus other rules to achieve a canonical form, possibly via completion techniques).

- Using Axioms (equations) vs Rules: If you allow axioms that aren’t oriented (i.e., treat some equations as bidirectional truths that aren’t used for computation), you gain flexibility in reasoning (you can prove things that the TRS might not be able to compute by itself). But you lose the algorithmic aspect – you then need a separate proof strategy. The ideal in a TRS approach is to find a confluent, terminating set of oriented axioms – then you don’t need separate reasoning tactics; rewriting is the reasoning. Achieving that is sometimes hard; not every theory has a nice confluent, terminating presentation. The Knuth-Bendix completion algorithm is a semi-decision procedure that tries to turn a set of axioms (equations) into a confluent, terminating TRS by adding appropriate rules (or it finds out it can’t)[43][44]. It will add rules to resolve critical pairs (like we manually did with d_X(0) rule) or orient equations consistent with a chosen well-founded order[45][46]. Completion may not succeed, but when it does, you get an axiom-free decision procedure for the equational theory.

- Different forms of rewriting systems: We focused on term rewriting (trees of symbols). There are other variants:

- String rewriting (Semi-Thue systems): where terms are strings of symbols and rules are like grammar productions replacing substrings. These are simpler in form (no tree structure), historically used in group theory word problems, etc. They are a special case of term rewriting (terms where all function symbols are binary infix “concatenation” basically). Many ideas (like confluence = Church-Rosser property) originated in that context.

- Graph rewriting: where terms can share subparts or even be cyclic graphs. Graph rewriting can represent things like pointer structures or optimize term rewriting by avoiding duplicating subterms. Graph rewriting is often used to implement term rewriting under the hood for efficiency (especially in functional programming implementations, to avoid recomputation). The core ideas of rules are similar but confluence/termination conditions are more involved with sharing.

- Conditional rewriting: rules that only apply if some condition holds (like an extra boolean guard or an equality that must be true). This adds expressive power but complicates analysis. Many practical systems allow conditional rules, but then termination/confluence analysis is harder.

- Higher-order rewriting: where you allow functions to be passed as arguments or have lambda abstractions in terms, etc. This merges with lambda calculus or higher-order logic. It’s more complex and usually beyond the first-order scope we stick to here.

Each of these variations introduces different benefits (e.g., graph rewriting can model circular or shared structures, conditional rules can encode more complex algorithms directly) and challenges (for instance, confluence criteria for higher-order or conditional systems are more complex).

For a first-order, unconditional TRS like we’ve mostly discussed, the benefit is its simplicity and strong theoretical properties. You can often decide many properties (not all – confluence, termination are undecidable in general, but lots of techniques exist). If your TRS is confluent and terminating, you have a very robust system (unique normal forms, decidable equality) which is great for both computation and proof. The shortcoming can be that you might need to augment the system with many rules to handle all cases (as seen, we had to add d_X(0)→0 to fix a confluence issue). Also, some mathematical properties are inherently hard to capture as terminating rewrite rules (like commutativity as noted). In practice, one might use a combination: a TRS for the computational core and some additional proof rules or meta-arguments for things like commutativity that are left out of the TRS to keep it terminating.

### Example: Symbolic Differentiation TRS

To illustrate another case study, consider a term rewriting system for symbolic differentiation (as seen briefly before). We use a function d_X(expr) to denote “the derivative of expr with respect to X.” Our signature includes constants (like numbers or other variable symbols), a binary + for addition, a binary * for multiplication, and the differentiation operator d_X( ). We can set up some rewrite rules for differentiation on this signature: 1. d_X(X) → 1   (the derivative of X w.r.t X is 1)[47]
2. d_X(Y) → 0   (the derivative of any other constant or variable Y (≠X) is 0)[48]
3. d_X(u + v) → d_X(u) + d_X(v)   (derivative of a sum is the sum of derivatives)[49]
4. d_X(u * v) → u * d_X(v) + v * d_X(u)   (product rule)[49]

These four rules form a TRS for differentiation. They are reminiscent of basic calculus rules. Notice they are all linear and left-hand sides are all distinct patterns, so we expect this system to be orthogonal and thus confluent[36] (which it is, as long as we treat X and Y as specific constants and not as variables in the rule sense – here X and Y are like two specific symbols where X is the one we differentiate with respect to, and Y is a generic other variable). The rule d_X(Y) → 0 covers the case for any symbol different from X; we cannot write it with a variable in the LHS without breaking the uniqueness (because d_X(X) and d_X(Y) would then overlap if X were also allowed to match the variable). That’s why in the formal list above X and Y are treated as meta-constants (we might call them tags indicating "same vs different variable").

Using these rules, the TRS can differentiate simple expressions. For example, starting with term d_X(X * (X + Y)): - We have a top-level d_X(u * v) pattern (with u = X and v = X+Y), so apply rule 4:
d_X(X * (X+Y)) → X * d_X(X+Y) + (X+Y) * d_X(X) (instantiating u and v)
- Now we have two d_X subterms: d_X(X+Y) and d_X(X). Both match rules: - For d_X(X+Y), rule 3 applies: d_X(X+Y) → d_X(X) + d_X(Y). So that part becomes d_X(X) + d_X(Y). - For d_X(X), rule 1 applies: d_X(X) → 1. After these, our expression becomes: X * (d_X(X) + d_X(Y)) + (X+Y) * 1. - Simplify each part: d_X(X) is 1 (rule 1), d_X(Y) is 0 (rule 2). So we get: X * (1 + 0) + (X+Y) * 1. - Now we can simplify arithmetic a bit if we have rules for that (like adding 0, multiplying by 1, etc., which we might include as well in an expanded TRS or just do as obvious simplification): - 1 + 0 → 1 (if we had a rule for eliminating 0 from sums)[50]. - (X+Y) * 1 → X+Y (if we had a rule for multiplication by 1 similarly). - After those simplifications: the result would be X * 1 + (X+Y) * 1 → X + (X+Y) → X + X + Y. If we also had a rule to simplify X+X (like combine like terms) or something, we might go further, but that’s beyond basic calculus rules. At least we got the derivative as X + (X+Y), which is indeed 2X + Y in usual algebra; our TRS in current form won’t combine X + X to 2X without extra rules, but it correctly applied differentiation rules.

This differentiation TRS is a nice example because it shows: - Use of multiple rules on the same operator (d_X has several cases). - An overlapping issue we discussed: originally we had d_X(X)→1 and one might think to write a single rule d_X(Z)→0 for “derivative of any Z that’s not X”, but you cannot easily do that without an explicit check because a single variable Z in LHS would also match X. So we use two rules with specific symbols to distinguish the cases, which is a bit of a hack but works in first-order TRS. (In a more expressive system, you might allow a condition “if the variable is not X” but plain TRS doesn’t allow that, so we enumerate cases). - The rules are orthogonal (no overlaps, since nothing can match both d_X(X) and d_X(Y) at the same time; and other rules are different function symbols or structures). - It’s terminating (each differentiation rule reduces the size of the expression: in rules 3 and 4, the d_X is applied to smaller subexpressions than the original). - It’s confluent (orthogonal implies confluence). So no matter which part you differentiate first, you end up with the same expanded derivative. - However, the system isn’t complete in itself to simplify the result fully algebraically (like it doesn’t combine the final result). TRS often are combined with additional rules for simplification, but adding those can introduce critical pairs as we saw with the 0-case. In fact, we saw adding v+0→v introduced a critical pair with differentiation that we resolved by adding d_X(0)→0[33]. If we were to add a rule like (X+Y)+(X+Y) → 2*(X+Y) for combining like terms, we’d step outside the first-order framework a bit (because introducing numeric coefficient 2* is a new operation), so one might not do that in TRS.

This example illustrates how TRS can encode logical or mathematical operations (differentiation rules mirror logical axioms of derivative in calculus) in a way that is computational – you can execute differentiation on symbolic expressions by rewriting[51].

### Handling of Axioms and Complex Properties

The user asked specifically about an “axiom-free” system. The examples we’ve shown (arithmetic, differentiation) use equations that we might normally call axioms (like x+0=x) but we oriented them as rules. In a sense, they are then not axioms you have to assume and prove with; they are rules you can directly apply as operations. This is exactly the power of TRS: it turns axioms into rewrite rules, thereby internalizing the logic as a form of computation. However, not every property can be so internalized easily: - Commutativity & Associativity: These are common axioms that, if oriented, typically lead to non-termination (e.g., x+y → y+x will flip forever). Often, TRS-based approaches either omit such axioms and accept that normal forms might not be unique unless you impose an external convention, or use a technique called * AC-rewriting where you build commutativity/associativity into the matching algorithm rather than as explicit rules. That’s advanced, but essentially the TRS operates modulo those axioms. - Identity elements (like 0 in addition or 1 in multiplication): We saw how to orient those to terminate (put the identity on the right so it disappears). That works well and is common. - Inverse elements (like subtraction or negatives in a group):* If you try to orient axioms like x + (-x) = 0, you have choices: x + (-x) → 0 works as a rule (it simplifies an expression with an inverse to 0). But the inverse axiom also implies a rewrite for moving terms across an equation. In TRS you might also include 0 → x + (-x) (to allow expansion) but that would introduce non-termination (0 could expand to x+(-x) and back via the other rule). So typically you orient one way and not the other. If you only do x+(-x)→0, it terminates (reduces complexity) and if combined with others carefully might be confluent, giving a way to cancel inverses. The tricky part is terms like (-x)+x would not match x+(-x) pattern (unless you had commutativity or separate rule for that ordering). This is where overlapping and needing more rules (or AC-matching) comes in. The Knuth-Bendix completion algorithm can sometimes handle these and give a confluent TRS for a theory like groups or rings by generating a bunch of critical pair rules, if it can succeed[43].

In summary, the design of a TRS involves balancing completeness of your rules (cover all needed cases) with termination and confluence. Each operator’s rules should make logical sense (correctness) and reduce complexity (termination), and multiple operators’ rules should cooperate without conflicts (confluence). It’s a bit of an art, but following the principles above (structural recursion, non-overlapping patterns, etc.) usually leads to a well-behaved system.

## Applications and Broader Context

Understanding TRS fundamentals is valuable because term rewriting underlies many areas in computer science and logic. Here are some notable applications and connections, along with examples:

- Algebraic Simplification (Computer Algebra Systems): Systems like Mathematica or Sympy use rewrite rules to simplify symbolic expressions. For example, a CAS might use rules like sin(π + x) → -sin(x) to simplify trigonometric expressions[52], or polynomial expansion and factorization rules. These are essentially term rewrites on the algebraic expression tree. TRSs can encode algebraic identities to systematically reduce expressions.

- Automated Theorem Proving (Equational Reasoning): TRS are heavily used in automated reasoning for equational logic. A confluent, terminating TRS serves as a decision procedure for the equality theory it represents[31]. Even if not fully decidable, rewrite rules can simplify goals in theorem provers. Many proof assistants (like Isabelle, Coq, Lean) have simplifiers that rely on user-provided rewrite rules (often called rewrite lemmas) to normalize terms during proofs[53]. Isabelle’s simplifier, for instance, takes a collection of rewrite rules and uses them to rewrite the goal formula repeatedly to try to solve it[53]. Lean and Coq have tactics like rewrite or simp that apply rewrite rules (lemmas marked as simplifications) across a term to simplify it. These are exactly TRS concepts in action: the lemmas are oriented as left→right rewrites (often terminating by construction, or at least they are applied with strategies to avoid loops), and the tactics apply them until normal form. Theorem provers also use Knuth-Bendix completion and paramodulation (a form of rewriting used with resolution) in their underlying engines to handle equalities.

- Formalized Mathematics and Proof Normalization: In logic, proof normalization (cut-elimination in sequent calculus, normalization in natural deduction, etc.) can be seen as a rewriting process on proof trees. Some logical systems are encoded as TRS where the proof or derivation is the term being rewritten to a normal form proof. This is more advanced, but it shows TRS even apply to rewriting proof objects to simplify them[54].

- Programming Languages (Functional Programming and Beyond): Functional programs can often be viewed as a set of rewrite rules (think of pattern matching function definitions). For example, in Haskell you might define:

- add x 0 = x
add x (S y) = S (add x y)

- This is essentially the same as our TRS for addition. The Haskell (or ML or Lean) runtime uses these equations to compute by rewriting (with some built-in strategy like lazy evaluation in Haskell or strict in ML). Therefore, understanding TRS gives insight into how these languages execute. In fact, the lambda calculus (which underlies functional languages) is a term rewriting system (with one rule: the β-reduction rule for function application). Languages like Maude or ASF+SDF or CafeOBJ explicitly use rewriting logic: you write equations (rewrite rules) as your program, and the engine reduces terms for you. Stratego/XT and Tom are other examples of term rewriting languages or tools used to transform programs and expressions.

- Compiler Optimizations: Compilers often use rewrite rules in optimizing code. An optimizing compiler might have rules like x*1 → x or more complex algebraic simplifications on an intermediate representation of a program[55]. They apply these rules to improve or simplify code. The term “peephole optimization” is essentially pattern-rewriting on small chunks of assembly code. Modern super-optimizers or equality saturation engines (like the Souper optimizer or the Rhodium system) use a saturation of rewrite rules to search for optimal forms of code. For instance, the technique of equality saturation stores many equivalent forms of a program (rewritten via various rules) in a common structure, then chooses the best one – rewriting is at the heart of generating those equivalents[2].

- Abstract Data Type Specifications: TRS can specify the behavior of abstract data types by equations. For example, a stack ADT might have constructors empty, push(elem, stack) and operations like top, pop with rules: top(push(x,s)) → x and pop(push(x,s)) → s (and perhaps an error term for top(empty)). Such a specification, if confluent and terminating, can serve as an executable specification – you can run it as a program, or reason about it equationally. Many formal specification languages (like OBJ, CafeOBJ, Maude) use this idea.

- AI and Symbolic Reasoning: In AI systems that involve symbolic manipulation (like theorem-solving, planning systems, or even some aspects of rewriting neural networks), TRS concepts appear. For instance, term rewriting is used in symbolic AI for rule-based expert systems and automated simplification of logical formulas. More directly, if the user is combining Lean (a theorem prover) with AI, they might be interested in how an AI could learn or propose rewrite steps (like using machine learning to guide which rewrite rule to apply).

- Rewriting Logic and Model Checking: There is a generalization of TRS known as rewriting logic (developed by José Meseguer and others) which uses rewriting as a model of computation and allows things like concurrent rewriting. Tools like Maude allow you to execute and model check systems specified as rewrite rules. This goes beyond our scope, but it’s a current research and practical area where TRS knowledge is fundamental.

In all these domains, the fundamental principles you’ve learned – how to design rules, ensure termination, ensure confluence (or cope without it by strategies), understanding normal forms – are critical. For example, if you’re using Lean’s simplifier, knowing that your set of simplification lemmas (rewrite rules) should be terminating (no infinite loops of rewriting) and confluent (or at least convergent to the intended normal form) will help you choose good lemmas. Lean avoids non-termination by not blindly applying rules that could loop, and often you use somewhat hierarchical strategies (like simplifier will not apply a lemma in reverse if that risks looping, etc.). By thinking like a TRS designer, you can ensure your rewrite-based automation in Lean or any system is robust.

## Conclusion

Term Rewriting Systems provide a powerful, foundational framework for computation and reasoning based on the simple idea of repeatedly replacing parts of an expression according to fixed rules. We covered the fundamentals: terms (expressions built from variables, constants, and operators)[7], rewrite rules (l → r transformations that replace a pattern with a result)[1], and how applying these rules constitutes a rewriting process that ideally ends in a normal form (fully simplified term). We emphasized two crucial properties – termination (the process always finishes) and confluence (the final result is unique, independent of rewrite order) – which make a TRS reliable as a computing system[29]. Achieving those often guides how we design the rules (using recursion that obviously terminates, non-overlapping patterns, etc.). We looked at examples like arithmetic and symbolic differentiation to see TRS design in action, showing how each operator’s rules are crafted and how the system can compute by rewriting.

We also discussed strategies and practical considerations: sometimes you need to control which rule to apply when, especially in complex systems or non-confluent settings, to avoid getting stuck or looping. In essence, term rewriting is computation by pattern matching – it’s a way to do all computation through applying patterns and replacements[56]. This viewpoint unifies many concepts in computer science: from functional programming (where function definitions = rewrite rules) to automated theorem proving (where proving equality = rewriting both sides to a common form)[2].

By learning TRS fundamentals, you gain insight into how to create axiom-free operator-based systems: everything is encoded in the operators and their rewrite rules. This knowledge should empower you to understand and design the “kernel” of such systems – specifying how each operator behaves, how they interact, and what rewriting strategies yield the desired outcome. Whether your goal is to implement a small programming language, build a simplifier for math expressions, or use a theorem prover’s automation effectively, the principles of term rewriting will be highly relevant.

Further Reading: If you wish to dive deeper, the classic text "Term Rewriting and All That" by Franz Baader and Tobias Nipkow is an excellent comprehensive resource[57]. There are also accessible tutorials[3][1] and lecture notes (like the CS294-260 notes we cited) that cover these topics with more examples and advanced techniques (e.g., critical pair analysis, completion algorithms[43], and strategy languages). With the fundamentals covered here, you should be well-prepared to explore those advanced materials and to start applying TRS concepts in practice. Good luck with your learning and your paper – may your rewrite rules be terminating and confluent, and your normal forms ever unique!

Sources:

- Klop, J.W. Term Rewriting Systems: a tutorial. Basic definitions of terms, signatures, and rules[58][59], plus examples of rewriting rules for arithmetic[38].

- Berkeley CS294-260 Lecture (2024). Introduction to term rewriting and its use in equational reasoning[60][11], including examples (symbolic differentiation)[51] and discussions of termination[24], confluence[20], and critical pairs[61].

- Number Analytics Blog (2025). Mastering Term Rewriting Systems – A Comprehensive Guide, covers TRS basics[1], properties (confluence/termination)[23], and applications in algebra, automated reasoning, and programming languages[62][63].

- Miller, Jimmy (2020). Introduction to Term Rewriting with Meander. Provides an intuitive explanation of rewriting as data transformation by pattern matching[64][56], illustrating the need for strategies in applying rules repeatedly[22].

[1] [3] [4] [5] [23] [25] [26] [29] [30] [52] [53] [54] [55] [62] [63]  Mastering Term Rewriting Systems

https://www.numberanalytics.com/blog/mastering-term-rewriting-systems

[2] [7] [11] [12] [15] [20] [24] [27] [28] [31] [32] [33] [34] [35] [36] [37] [39] [40] [41] [43] [44] [45] [46] [47] [48] [49] [50] [51] [57] [60] [61]  Term Rewriting | CS294-260

https://inst.eecs.berkeley.edu/~cs294-260/sp24/2024-01-22-term-rewriting

[6] [8] [9] [10] [14] [16] [17] [18] [19] [38] [58] [59] CWI Scanprofile/PDF/300

https://ir.cwi.nl/pub/6129/6129D.pdf

[13] [22] [42] [56] [64] Introduction to Term Rewriting with Meander

http://jimmyhmiller.com/meander-rewriting

[21] Confluence (abstract rewriting) - Wikipedia

https://en.wikipedia.org/wiki/Confluence_(abstract_rewriting)

